# -*- coding: utf-8 -*-
"""DynamicBayesian_share.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EMknchjkZ-_WWbxDb45qFtLDMSoEAoiI

# Cosyne2025: A Computational Model of Canonical Cortical Microcircuits
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

import pandas as pd
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import pearsonr
import seaborn as sns

"""### Markov chain"""

class Markov:
    """Class for a Markov chain"""

    def __init__(self, ptr):
        """Create a new environment"""
        self.ptr = ptr  # transition matrix p(x'|x)
        self.Ns = len(ptr)  # number of states

    def sample(self, x0=0, step=1):
        """generate a sample sequence from x0"""
        seq = np.zeros(step+1, dtype=int) # sequence buffer
        seq[0] = x0
        for t in range(step):
            pt1 = self.ptr[:, seq[t]] # prob. of new states
            seq[t+1] = np.random.choice(self.Ns, p=pt1) # sample
        return seq

    def forward(self, p0, step=1):
        """forward message from initial distribution p0"""
        alpha = np.zeros((step+1, self.Ns)) # priors
        alpha[0] = p0  # initial distribution
        for t in range(step):
            alpha[t+1] = self.ptr @ alpha[t]
        return alpha

    def backward(self, obs, step=1):
        """backward message from terminal observaion"""
        beta = np.zeros((step+1, self.Ns)) # likelihoods
        beta[-1] = obs  # final observation
        for t in range(step, 0, -1): # backward toward t=0
            beta[t-1] = beta[t] @ self.ptr
        return beta

    def posterior(self, p0, obs, step):
        """forward-backward algorithm"""
        alpha = self.forward(p0, step)
        beta = self.backward(obs, step)
        post = alpha*beta
        for t in range(step+1):
            post[t] = post[t]/sum(post[t])  # normalize
        return post

"""## Dynamic Bayesian Inference

### Hidden Markov model
"""

class HMM(Markov):
    """Hidden Markov model"""

    def __init__(self, ptr, pobs):
        """Create HMM with transition and observation models"""
        super().__init__(ptr)
        self.pobs = pobs  # observation model
        self.No = len(pobs)  # number of observations
        self.pst = np.ones(self.Ns)/self.Ns  # state distribution
        self.pred = np.zeros(self.Ns)  # predictive distribution

    def sample(self, x0=0, step=10):
        """generate a sample sequence from x0"""
        xt = np.zeros(step, dtype=int) # state sequence
        yt = np.zeros(step, dtype=int) # observation sequence
        xt[0] = x0
        po = self.pobs[:, x0] # observation probabilities for initial state
        yt[0] = np.random.choice(self.No, p=po) # generate observation
        for t in range(1, step):
            ps = self.ptr[:, xt[t-1]]  # prob. of new states
            xt[t] = np.random.choice(self.Ns, p=ps) # state transition
            po = self.pobs[:, xt[t]]   # observation probabilities
            yt[t] = np.random.choice(self.No, p=po) # generate observation
        return xt, yt

    def predict(self):
        """predictive prior by transition model"""
        self.pred = self.ptr @ self.pst # L5IT = L3IT @ L5PT

    def update(self, obs):
        """update posterior by observation"""
        prl = self.pobs[obs]*self.pred # likelihood*prior # L2IT*L5IT
        self.pst = prl/sum(prl)  #normalize # L5PT <- L5SST

    def reset(self):
        """reset state probability"""
        self.pst = np.ones(self.Ns)/self.Ns  # uniform

    def step(self, obs):
        """one step of dynamic bayesian inference"""
        self.predict()
        self.update(obs)
        max_index = np.argmax(self.pst) # L6IT <- L6SST
        map_estimate = np.zeros_like(self.pst)
        map_estimate[max_index] = 1
        obs_one_hot = np.zeros(self.No)
        obs_one_hot[obs] = 1
        return self.pst, obs_one_hot, self.pobs[obs], self.ptr, self.pred, map_estimate

    def simulate_observations(self, time_steps=100, time_bin=9.0, peristimulus=700.0):
        """
        Simulate a sequence of observations and output the start times and durations when observation 1 occurs,
        as well as the state transitions and observation sequence.

        Parameters:
        time_steps (int): Number of time steps in the simulation (default is 100)
        time_bin (float): Time width of each bin (default is 9.0)
        peristimulus (float): Time added to the start of the observation (default is 700.0)

        Returns:
        start_times (list): List of start times when observation 1 begins
        durations (list): List of durations when observation 1 continues
        xt (ndarray): List of state transitions
        yt (ndarray): List of observations
        """
        ns = self.ptr.shape[0]  # number of states
        no = self.pobs.shape[0]  # number of observations

        # Randomly select an initial state
        current_state = np.random.choice(ns)

        # Initialize lists to store state transitions and observations
        xt = np.zeros(time_steps, dtype=int)
        yt = np.zeros(time_steps, dtype=int)
        xt[0] = current_state

        for t in range(time_steps):
            # Generate observation based on the current state
            yt[t] = np.random.choice(no, p=self.pobs[:, current_state])

            if t < time_steps - 1:
                # Perform state transition
                current_state = np.random.choice(ns, p=self.ptr[:, current_state])
                xt[t + 1] = current_state

        # Calculate the start times and durations when observation 1 occurs
        start_times = []
        durations = []
        current_duration = 0
        in_observation_0 = False  # Whether observation 1 is ongoing

        for t in range(time_steps):
            if yt[t] == 0:
                if not in_observation_0:
                    # Record the start time when observation 0 begins
                    start_times.append(t)
                    in_observation_0 = True
                current_duration += 1
            else:
                if in_observation_0:
                    # Record the duration when observation 1 ends
                    durations.append(current_duration)
                    current_duration = 0
                    in_observation_0 = False

        # If observation 1 has not ended, record the final duration
        if in_observation_0:
            durations.append(current_duration)

        # Multiply the times by the time_bin
        start_times = (np.array(start_times) * time_bin + peristimulus).tolist()
        durations = (np.array(durations) * time_bin).tolist()

        return start_times, durations, xt, yt

"""There are only four potential states, with a 0.9 probability of staying and a 0.1 probability of moving to a different state."""

# random walk on a ring
ns = 4 # ring size
ps = 0.1  # shift probability
Ptr = np.zeros((ns, ns))  # transition matrix
for i in range(ns):
    Ptr[i,i] = 1 - ps
    Ptr[(i+1)%ns, i] = ps
plt.imshow(Ptr)
plt.xlabel("State"); plt.ylabel("Next state");
plt.title("State Transition Model P(next state|state)")
plt.colorbar()
plt.show()

"""Observations are obtained at 0.7 from one state, but at 0.1 from a different ones."""

# Blurred intermittent observation model

# Current parameters for 4 states
no = 4
po = 0.7
Pobs = np.zeros((no, ns))  # p(obs|state)
Pobs[0,0] = Pobs[1,1] = Pobs[2,2] = Pobs[3,3] = po  # correct information
Pobs[1:4,0] = Pobs[0,1] = Pobs[2:4,1] = Pobs[0:2,2] = Pobs[3,2] = Pobs[0:3,3] = (1-po)/3 # wrong information

# 観測モデルの可視化
plt.imshow(Pobs, cmap='viridis')
plt.xlabel("State")
plt.ylabel("Observation")
plt.title("Observation Model P(obs|state)")
plt.colorbar()
plt.show()

# crate a HMM
ring = HMM(Ptr, Pobs)

# sample a state trajectory and observations
T = 100

# Call the function to get the start times and durations of observation 1
start_times, durations, xt, yt = ring.simulate_observations(T)

# Display results
print("Start times when observation 1 begins:", start_times)
print("Durations for which observation 1 continued:", durations)
plt.plot(xt, label='State')
plt.plot(yt, 'ro', label='Observation')
plt.legend()

"""Code for Dynamice Bayesian inference"""

# Dynamic Bayesian inference in HMM
post = np.zeros((T, ns))  # posterior trajectory
observation = np.zeros((T, no))
likelihood = np.zeros((T, ns))
prediction =  np.zeros((ns, ns))
predictiveprior = np.zeros((T, ns))
mapestimate = np.zeros((T, ns))

ring.reset()
for t in range(T):
    post[t], observation[t], likelihood[t], prediction, predictiveprior[t], mapestimate[t] = ring.step(yt[t])

# Convert to 2D one-hot vector
prediction_array = predictiveprior

# Create a new figure for the line plots
fig, ax = plt.subplots(figsize=(10, 6))

# Plot Likelihood as a line plot
ax.plot(likelihood[:, 0], color='red', label='Likelihood', linewidth=1.5)

# Plot prediction array as a line plot
ax.plot(prediction_array[:, 0], color='green', label='Prediction', linewidth=1.5)

# Plot Likelihood as a line plot
ax.plot(likelihood[:, 0], color='red', label='Likelihood', linewidth=1.5)

# Plot Predictive Prior as a line plot
ax.plot(predictiveprior[:, 0], color='orange', label='Predictive Prior', linewidth=1.5)

# Plot Posterior Trajectory (post) as a line plot
ax.plot(post[:, 0], color='blue', label='Post', linewidth=1.5)

# Plot MAP Trajectory (map) as a line plot
ax.plot(mapestimate[:, 0], color='cyan', label='MAP Trajectory', linewidth=1.5)

# Plot True State as a line plot
ax.plot(xt, color='purple', label='True State', linewidth=1.5)

# Add titles and labels
ax.set_title('Line Plots of All Variables', fontsize=16)
ax.set_xlabel('t', fontsize=12)
ax.set_ylabel('Value', fontsize=12)

# Add a legend to differentiate the plots
ax.legend(loc='upper right')

# Add grid for better readability
ax.grid(True)

# Show the plot
plt.tight_layout()
plt.show()

# Create separate figures for each plot
fig, ax = plt.subplots(6, 1, figsize=(10, 18))

# Plot Likelihood as a line plot
ax[0].plot(likelihood[:, 0], color='red', linewidth=1.5)
ax[0].set_title('Likelihood', fontsize=32)
ax[0].set_xlabel('t', fontsize=24)
ax[0].set_ylabel('Value', fontsize=24)
ax[0].grid(True)

# Plot Prediction array as a line plot
ax[1].plot(prediction_array[:, 0], color='green', linewidth=1.5)
ax[1].set_title('Prediction', fontsize=32)
ax[1].set_xlabel('t', fontsize=24)
ax[1].set_ylabel('Value', fontsize=24)
ax[1].grid(True)

# Plot Likelihood as a line plot
ax[2].plot(likelihood[:, 0], color='red', linewidth=1.5)
ax[2].set_title('Likelihood', fontsize=32)
ax[2].set_xlabel('t', fontsize=24)
ax[2].set_ylabel('Value', fontsize=24)
ax[2].grid(True)

# Plot Predictive Prior as a line plot
ax[3].plot(predictiveprior[:, 0], color='orange', linewidth=1.5)
ax[3].set_title('Predictive Prior', fontsize=32)
ax[3].set_xlabel('t', fontsize=24)
ax[3].set_ylabel('Value', fontsize=24)
ax[3].grid(True)

# Plot Posterior Trajectory (post) as a line plot
ax[4].plot(post[:, 0], color='blue', linewidth=1.5)
ax[4].set_title('Posterior', fontsize=32)
ax[4].set_xlabel('t', fontsize=24)
ax[4].set_ylabel('Value', fontsize=24)
ax[4].grid(True)

# Plot MAP Trajectory (map) as a line plot
ax[5].plot(mapestimate[:, 0], color='cyan', linewidth=1.5)
ax[5].set_title('MAP', fontsize=32)
ax[5].set_xlabel('t', fontsize=24)
ax[5].set_ylabel('Value', fontsize=24)
ax[5].grid(True)

# Adjust layout to prevent overlapping
plt.tight_layout()

# Show the plot
plt.show()

"""# Parameter Definitions"""

import numpy as np
import torch
import matplotlib.pyplot as plt
import torch.nn as nn

# Size of the neuron population (defined as 2^n)
n = 7  # Example: n=3 (neuron population size is 2^n)
num_neurons = 2**n

# Time steps for the simulation
time_steps = 100
dt = 1   # Time step (ms)

# Parameters for the LIF model
tau_m = 20  # Time constant (ms)
V_reset = 0.0  # Reset potential
V_threshold = 0.5   # Firing threshold
w = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  # Synaptic weights for L4->L2IT, prevL5PT->L3IT, obs->L4E, L3IT->L5IT, L2IT*L5IT->L5PT, L5PT->L6IT
background_rate = [0.12, 0.34, 0.12, 0.12, 0.09, 0.88]  # Background firing rates for L2IT, L5IT, L5PT, L6IT, L5SST, L6SST
beta = 12   # Sharpness of the spike function gradient

# Synaptic weights for inhibitory neurons
w_L5PT_to_Inh = 0.80  # Weight from L5PT to inhibitory neurons
w_Inh_to_L5PT = 0.17  # Inhibitory effect from inhibitory neurons to L5PT
w_L6IT_to_Inh = 1.5   # Weight from L6IT to inhibitory neurons (as needed)
w_Inh_to_L6IT = 0.18  # Inhibitory effect from inhibitory neurons to L6IT

# Generate training data
num_samples = 1000

# Simulation parameters
duration = 1  # ms
duration_steps = int(duration / dt)

# Set training parameters
input_size = no
num_hidden = num_neurons
nb_epochs = 10
lr = 0.01
batch_size = 32

# Function to update membrane potential in LIF neurons
def lif_update(V, I, dt, tau_m, V_reset, V_threshold):
    alpha = np.exp(-dt / tau_m)  # Decay rate based on time constant
    V = V * alpha + I  # Update membrane potential
    spikes = V >= V_threshold  # Check if neurons fired
    V[spikes] = V_reset  # Reset fired neurons
    return V, spikes

# Function to generate Poisson spikes
def poisson_spikes(rate, size, time_steps):
    return torch.poisson(rate * torch.ones((size, time_steps)))

"""# Training the Observation Model

Generating training data

*   Randomly select states and generate corresponding observations based on the observation model
*   Generate state sequences based on the state transition model
"""

# Randomly select states
states = np.random.choice(ns, size=num_samples)
observations = np.zeros(num_samples, dtype=int)

# Generate observations based on the states
for i in range(num_samples):
    state = states[i]
    obs_probs = Pobs[:, state]
    observations[i] = np.random.choice(no, p=obs_probs)

# Convert to PyTorch tensors
states_tensor = torch.tensor(states, dtype=torch.float)
observations_tensor = torch.tensor(observations, dtype=torch.float)

"""Generating Input Spikes

Generate input spike trains corresponding to observations.

*   For each observation, generate a spike train where the corresponding input neuron has a high firing rate.
*   Other neurons have a low background firing rate.
"""

# Function to generate input spikes
def generate_input_spikes(observations, duration_steps, dt, high_rate=1000.0, low_rate=0.0):
    num_samples = len(observations)
    input_size = no  # Number of input neurons (equal to the number of observations)
    input_spikes = torch.zeros((num_samples, duration_steps, input_size))
    for i in range(num_samples):
        observation = observations[i]
        for neuron in range(input_size):
            rate = high_rate if neuron == observations[i] else low_rate
            spike_probs = rate * dt / 1000.0  # Convert Hz to probability
            spikes = torch.bernoulli(torch.full((duration_steps,), spike_probs))
            input_spikes[i, :, neuron] = spikes
    return input_spikes

# Generate input spikes
input_spikes = generate_input_spikes(observations, duration_steps, dt)

# Generate target outputs
# For each state, activate the corresponding neuron population
target_outputs = torch.zeros((num_samples, num_neurons))
neurons_per_state = num_neurons // ns  # Number of neurons representing each state
for i in range(num_samples):
    state = states[i]
    start_idx = state * neurons_per_state
    end_idx = start_idx + neurons_per_state
    target_outputs[i, start_idx:end_idx] = 1.0

"""Defining the Spike Function

Create a function to define spike firing.

*   Define a differentiable approximation of the spike function to approximate the nonlinearity of spikes.
"""

class SurrGradSpike(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        out = torch.zeros_like(input)
        out[input > 0] = 1.0
        return out
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad = grad_output * beta * torch.sigmoid(beta * input) * (1 - torch.sigmoid(beta * input))
        return grad

spike_fn = SurrGradSpike.apply

"""Initializing Weights

Initialize the weights of the neural network.
"""

def init_weight_matrices(input_size, num_hidden, num_neurons):
    # Weights from the input layer to the hidden layer
    W1 = nn.Parameter(torch.empty((input_size, num_hidden), requires_grad=True))
    nn.init.uniform_(W1, -1 / np.sqrt(input_size), 1 / np.sqrt(input_size))
    # Weights from the hidden layer to the output layer
    W2 = nn.Parameter(torch.empty((num_hidden, num_neurons), requires_grad=True))
    nn.init.uniform_(W2, -1 / np.sqrt(num_hidden), 1 / np.sqrt(num_hidden))
    return W1, W2


"""Definition of SNN

Define the forward propagation of the spiking neural network.

*   Calculate spike firing and membrane potential updates at each time step.
*   Define a network that includes both the hidden layer and the output layer.
"""

def snn(input_spikes, W1, W2, tau=20, dt=1):
    batch_size = input_spikes.size(0)
    num_hidden = W1.size(1)
    num_neurons = W2.size(1)
    # Initialize the hidden layer
    v = torch.zeros((batch_size, num_hidden))
    s = torch.zeros((batch_size, num_hidden))
    s_rec = []
    h1 = torch.einsum("abc,cd->abd", (input_spikes, W1))
    alpha = np.exp(-dt / tau)
    for t in range(h1.size(1)):
        v = alpha * v + h1[:, t, :]
        s = spike_fn(v - 1.0)  # Firing threshold is 1.0
        s_rec.append(s)
        v = v * (1 - s)  # Reset the neurons that fired
    s_rec = torch.stack(s_rec, dim=1)
    # Initialize the output layer
    v = torch.zeros((batch_size, num_neurons))
    v_rec = []
    h2 = torch.einsum("abc,cd->abd", (s_rec, W2))
    for t in range(h2.size(1)):
        v = alpha * v + h2[:, t, :]
        s = spike_fn(v - 1.0)  # Firing threshold is 1.0
        v_rec.append(s)
        v = v * (1 - s)  # Reset the neurons that fired
    v_rec = torch.stack(v_rec, dim=1)
    return v_rec

"""Training the Network

Train the network and optimize the weights.

*   Optimize the network weights using the training data.
*   Use negative log-likelihood loss as the loss function.
*   Output the loss at each epoch and visualize the training progress.
"""

# Initialize weights
W1, W2 = init_weight_matrices(input_size, num_hidden, num_neurons)

# Define loss function and optimizer
optimizer = torch.optim.Adam([W1, W2], lr=lr) #, weight_decay=weight_decay)
loss_fn = nn.MSELoss()

# Create a data loader
from torch.utils.data import TensorDataset, DataLoader, random_split
dataset = TensorDataset(input_spikes, target_outputs)

# Split the dataset into training and testing (80:20 ratio)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# === Training Loop ===
loss_hist = []
test_loss_hist = []
for epoch in range(nb_epochs):
    local_loss = []; local_m = []; local_x = []
    for x_local, y_local in train_loader:
        local_x.append(torch.mean(x_local, dim=1))
        # Run the network
        output = snn(x_local, W1, W2)
        # Calculate the mean output
        tensor_mean = torch.mean(output, dim=1)
        # Process to split tensor based on ns and calculate means
        chunks_tensor_mean = torch.chunk(tensor_mean, ns, dim=1)
        chunks_y_local = torch.chunk(y_local, ns, dim=1)

        # Calculate the means of each chunk
        means = [chunk.mean(dim=1, keepdim=True) for chunk in chunks_tensor_mean]
        y_locals = [chunk.mean(dim=1, keepdim=True) for chunk in chunks_y_local]

        # Concatenate the means and output
        m = torch.cat(means, dim=1)
        y_local_m = torch.cat(y_locals, dim=1)
        local_m.append(m)
        # Calculate the loss
        loss = loss_fn(m, y_local_m)
        local_loss.append(loss.item())
        # Update gradients
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    loss_hist.append(np.mean(local_loss))

    # Evaluation on the test data
    with torch.no_grad():
        test_local_loss = []; test_local_m = []; test_local_x = []
        for x_test, y_test in test_loader:
            test_local_x.append(torch.mean(x_test, dim=1))
            output_test = snn(x_test, W1, W2)
            tensor_mean_test = torch.mean(output_test, dim=1)
            # Process to split tensor based on ns and calculate means (test data)
            chunks_tensor_mean_test = torch.chunk(tensor_mean_test, ns, dim=1)
            chunks_y_test = torch.chunk(y_test, ns, dim=1)

            # Calculate the means of each chunk
            means_test = [chunk.mean(dim=1, keepdim=True) for chunk in chunks_tensor_mean_test]
            y_tests = [chunk.mean(dim=1, keepdim=True) for chunk in chunks_y_test]

            # Concatenate the means and output
            m_test = torch.cat(means_test, dim=1)
            y_test_m = torch.cat(y_tests, dim=1)

            test_local_m.append(m_test)
            loss_test = loss_fn(m_test, y_test_m)
            test_local_loss.append(loss_test.item())
    test_loss_hist.append(np.mean(test_local_loss))

    print(f"Epoch {epoch + 1}: Loss = {np.mean(local_loss):.5f}, Test Loss = {np.mean(test_local_loss):.5f}")

    # Retrieve outputs at epoch 0 (first epoch)
    if epoch == 0:
        avg_firing_rates_epoch0 = local_m
        obs_epoch0 = local_x
        test_avg_firing_rates_epoch0 = test_local_m
        test_obs_epoch0 = test_local_x
    # Retrieve outputs at the last epoch
    if epoch == nb_epochs - 1:
        avg_firing_rates_last_epoch = local_m
        obs_last_epoch = local_x
        test_avg_firing_rates_last_epoch = test_local_m
        test_obs_last_epoch = test_local_x

# Plot the loss
plt.figure(figsize=(6, 4))
plt.plot(loss_hist, label='Training Loss')
plt.plot(test_loss_hist, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Test Loss over Epochs')
plt.show()


def plot_mean_values(local_x, local_m, title='Mean Values by Group'):
    """
    local_x: A list containing tensors of size n*2 (e.g., [[0,1], [1,0], ...])
    local_m: A list containing tensors of size n*8
    """
    import torch
    import matplotlib.pyplot as plt

    # Concatenate the elements of local_x into an n*2 tensor
    x_tensor = torch.cat(local_x, dim=0).view(-1, local_x[0].shape[-1])

    # Concatenate the elements of local_m into an n*8 tensor
    m_tensor = torch.cat(local_m, dim=0).view(-1, local_m[0].shape[-1])

    # Separate into two groups based on the first element of x_tensor
    group1_indices = torch.where(x_tensor[:, 0] == 0)[0]
    group2_indices = torch.where(x_tensor[:, 0] == 1)[0]

    # Compute the mean for each group
    if group1_indices.numel() > 0:
        mean_group1 = m_tensor[group1_indices].mean(dim=0)
    else:
        mean_group1 = torch.zeros(m_tensor.shape[1])

    if group2_indices.numel() > 0:
        mean_group2 = m_tensor[group2_indices].mean(dim=0)
    else:
        mean_group2 = torch.zeros(m_tensor.shape[1])

    # Convert tensors to numpy arrays
    mean_group1_np = mean_group1.detach().numpy()
    mean_group2_np = mean_group2.detach().numpy()

    # Plot mean values as bar graph
    fig, ax = plt.subplots()
    bar_width = 0.35
    x_indices = np.arange(mean_group1_np.shape[0])

    ax.bar(x_indices, mean_group1_np, bar_width, label='[0,1]')
    ax.bar(x_indices + bar_width, mean_group2_np, bar_width, label='[1,0]')

    ax.set_xlabel('Element Index')
    ax.set_ylabel('Mean Value')
    ax.set_title(title)
    ax.legend()

    plt.show()

# Plot mean values (training data)
plot_mean_values(obs_epoch0, avg_firing_rates_epoch0, title='Mean Values by Group (Epoch 0 - Training)')
plot_mean_values(obs_last_epoch, avg_firing_rates_last_epoch, title='Mean Values by Group (Last Epoch - Training)')

# Plot mean values (test data)
plot_mean_values(test_obs_epoch0, test_avg_firing_rates_epoch0, title='Mean Values by Group (Epoch 0 - Test)')
plot_mean_values(test_obs_last_epoch, test_avg_firing_rates_last_epoch, title='Mean Values by Group (Last Epoch - Test)')


"""# Training the State Transition Model

1. Generate training data for the state transition model
"""

# Randomly select previous states
prev_states = np.random.choice(ns, size=num_samples)
next_states = np.zeros(num_samples, dtype=int)

# Generate next states based on the state transition model
for i in range(num_samples):
    state = prev_states[i]
    next_states[i] = np.random.choice(ns, p=Ptr[:, state])

# Convert to PyTorch tensors
prev_states_tensor = torch.tensor(prev_states, dtype=torch.float)
next_states_tensor = torch.tensor(next_states, dtype=torch.float)

"""2. Define and Train the SNN

Generate input spikes
"""

# Function to generate input spikes (representing the previous states)
def generate_input_spikes_states(states, duration_steps, dt, num_neurons, neurons_per_state, high_rate=1000.0, low_rate=0.0):
    num_samples = len(states)
    input_spikes = torch.zeros((num_samples, duration_steps, num_neurons))
    for i in range(num_samples):
        state = states[i]
        start_idx = state * neurons_per_state
        end_idx = start_idx + neurons_per_state
        for neuron in range(num_neurons):
            rate = high_rate if start_idx <= neuron < end_idx else low_rate
            spike_probs = rate * dt / 1000.0  # Convert Hz to probability
            spikes = torch.bernoulli(torch.full((duration_steps,), spike_probs))
            input_spikes[i, :, neuron] = spikes
    return input_spikes

# Function to generate target output spikes (representing the next states)
def generate_target_output_spikes_states(states, duration_steps, dt, num_neurons, neurons_per_state, high_rate=1000.0, low_rate=0.0):
    num_samples = len(states)
    target_spikes = torch.zeros((num_samples, duration_steps, num_neurons))
    for i in range(num_samples):
        state = states[i]
        start_idx = state * neurons_per_state
        end_idx = start_idx + neurons_per_state
        for neuron in range(num_neurons):
            rate = high_rate if start_idx <= neuron < end_idx else low_rate
            spike_probs = rate * dt / 1000.0
            spikes = torch.bernoulli(torch.full((duration_steps,), spike_probs))
            target_spikes[i, :, neuron] = spikes
    return target_spikes

"""Generate input spikes and target output spikes"""

# Generate input spikes (representing the previous states)
input_spikes_states = generate_input_spikes_states(prev_states, duration_steps, dt, num_neurons, neurons_per_state)
# Generate target output spikes (representing the next states)
target_output_spikes = generate_target_output_spikes_states(next_states, duration_steps, dt, num_neurons, neurons_per_state)


"""# Definition of the SNN"""

class SurrGradSpike(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        out = torch.zeros_like(input)
        out[input > 0] = 1.0
        return out
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad = grad_output * beta * torch.sigmoid(beta * input) * (1 - torch.sigmoid(beta * input))
        return grad

spike_fn = SurrGradSpike.apply

# Definition of the SNN
def snn_transition(input_spikes, W1, W2, tau=20, dt=1):
    batch_size = input_spikes.size(0)
    num_hidden = W1.size(1)
    num_neurons = W2.size(1)
    # Initialize the hidden layer
    v = torch.zeros((batch_size, num_hidden))
    s = torch.zeros((batch_size, num_hidden))
    s_rec = []
    h1 = torch.einsum("abc,cd->abd", (input_spikes, W1))
    alpha = np.exp(-dt / tau)
    for t in range(h1.size(1)):
        v = alpha * v + h1[:, t, :]
        s = spike_fn(v - 1.0)  # Firing threshold is 1.0
        s_rec.append(s)
        v = v * (1 - s)  # Reset neurons that have fired
    s_rec = torch.stack(s_rec, dim=1)  # Size: [batch_size, time_steps, num_hidden]
    # Initialize the output layer
    v = torch.zeros((batch_size, num_neurons))
    v_rec = []
    h2 = torch.einsum("abc,cd->abd", (s_rec, W2))  # Size of h2: [batch_size, time_steps, num_neurons]
    for t in range(h2.size(1)):
        v = alpha * v + h2[:, t, :]
        s = spike_fn(v - 1.0)  # Firing threshold is 1.0
        v_rec.append(s)  # Record spike outputs
        v = v * (1 - s)  # Reset neurons that have fired
    v_rec = torch.stack(v_rec, dim=1)  # Size: [batch_size, time_steps, num_neurons]
    return v_rec  # Return spike sequences

"""Weight initialization"""

def init_weight_matrices_transition(input_size, num_hidden, num_neurons):
    W1 = nn.Parameter(torch.empty((input_size, num_hidden), requires_grad=True))
    nn.init.uniform_(W1, -1 / np.sqrt(input_size), 1 / np.sqrt(input_size))
    W2 = nn.Parameter(torch.empty((num_hidden, num_neurons), requires_grad=True))
    nn.init.uniform_(W2, -1 / np.sqrt(num_hidden), 1 / np.sqrt(num_hidden))
    return W1, W2

# Parameter settings
input_size = num_neurons # Number of input neurons (number of neurons in L3IT)
num_hidden = num_neurons  # Number of neurons in the hidden layer (adjust accordingly)

"""Training the network"""

# Initialize weights
W1_L3IT, W2_L3IT = init_weight_matrices_transition(input_size, num_hidden, num_neurons)

# Define the loss function and optimizer
optimizer_L3IT = torch.optim.Adam([W1_L3IT, W2_L3IT], lr=lr)
loss_fn = nn.MSELoss()

# Create the data loader
from torch.utils.data import TensorDataset, DataLoader, random_split
dataset_transition = TensorDataset(input_spikes_states, target_output_spikes)

# Split the dataset into training and test sets (80:20 ratio)
train_size = int(0.8 * len(dataset_transition))
test_size = len(dataset_transition) - train_size
train_dataset, test_dataset = random_split(dataset_transition, [train_size, test_size])

# Create the data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Training loop
loss_hist_transition = []
test_loss_hist_transition = []

for epoch in range(nb_epochs):
    local_loss = []; local_m = []; local_x = []
    for x_local, y_local in train_loader:
        local_x.append(torch.mean(x_local, dim=1))
        # Run the network
        output = snn_transition(x_local, W1_L3IT, W2_L3IT)
        # Compute the mean of the outputs
        tensor_mean = torch.mean(output, dim=1); y_local_mean = torch.mean(y_local, dim=1)
        # Split the tensor based on ns and compute the mean for each chunk
        chunks_tensor_mean = torch.chunk(tensor_mean, ns, dim=1)
        chunks_y_local_mean = torch.chunk(y_local_mean, ns, dim=1)

        # Compute the mean of each chunk
        means = [chunk.mean(dim=1, keepdim=True) for chunk in chunks_tensor_mean]
        y_locals = [chunk.mean(dim=1, keepdim=True) for chunk in chunks_y_local_mean]

        # Concatenate the means and output
        m = torch.cat(means, dim=1)
        y_local_m = torch.cat(y_locals, dim=1)

        local_m.append(m)
        # Compute the loss
        loss = loss_fn(m, y_local_m)
        local_loss.append(loss.item())
        # Update gradients
        optimizer_L3IT.zero_grad()
        loss.backward()
        optimizer_L3IT.step()
    loss_hist_transition.append(np.mean(local_loss))

    # Evaluate on the test data
    with torch.no_grad():
        test_local_loss = []; test_local_m = []; test_local_x = []
        for x_test, y_test in test_loader:
            test_local_x.append(torch.mean(x_test, dim=1))
            output_test = snn_transition(x_test, W1_L3IT, W2_L3IT)
            tensor_mean_test = torch.mean(output_test, dim=1); y_test_mean = torch.mean(y_test, dim=1)
            # Split the tensor based on ns and compute the mean for each chunk (test data)
            chunks_tensor_mean_test = torch.chunk(tensor_mean_test, ns, dim=1)
            chunks_y_test_mean = torch.chunk(y_test_mean, ns, dim=1)

            # Compute the mean of each chunk
            means_test = [chunk.mean(dim=1, keepdim=True) for chunk in chunks_tensor_mean_test]
            y_tests = [chunk.mean(dim=1, keepdim=True) for chunk in chunks_y_test_mean]

            # Concatenate the means and output
            m_test = torch.cat(means_test, dim=1)
            y_test_m = torch.cat(y_tests, dim=1)

            test_local_m.append(m_test)
            loss_test = loss_fn(m_test, y_test_m)
            test_local_loss.append(loss_test.item())
    test_loss_hist_transition.append(np.mean(test_local_loss))

    print(f"Epoch {epoch + 1}: Loss = {np.mean(local_loss):.5f}")

    # Get outputs at epoch 0 (first epoch)
    if epoch == 0:
        avg_firing_rates_epoch0 = local_m
        obs_epoch0 = local_x
        test_avg_firing_rates_epoch0 = test_local_m
        test_obs_epoch0 = test_local_x
    # Get outputs at the last epoch
    if epoch == nb_epochs - 1:
        avg_firing_rates_last_epoch = local_m
        obs_last_epoch = local_x
        test_avg_firing_rates_last_epoch = test_local_m
        test_obs_last_epoch = test_local_x

# Plot the loss
plt.figure(figsize=(6, 4))
plt.plot(loss_hist_transition, label='Training Loss')
plt.plot(test_loss_hist_transition, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss over Epochs for L3IT')
plt.show()


def plot_mean_values(local_x, local_m, title='Mean Values by Group'):
    """
    local_x: A list containing tensors of length n (e.g., [[0,0,0,0,1,1,1,1], [1,1,1,1,0,0,0,0], ...])
    local_m: A list containing tensors of length n
    """
    import torch
    import matplotlib.pyplot as plt

    # Concatenate elements of local_x into an n*8 tensor
    x_tensor = torch.cat(local_x, dim=0).view(-1, local_x[0].shape[-1])

    # Concatenate elements of local_m into an n*8 tensor
    m_tensor = torch.cat(local_m, dim=0).view(-1, local_m[0].shape[-1])

    # Calculate the mean of local_m for [0,0,0,0,1,1,1,1] and [1,1,1,1,0,0,0,0]
    m_group1 = []
    m_group2 = []

    for i, x in enumerate(x_tensor):
        if x[0] == 0:
            m_group1.append(m_tensor[i])
        elif x[0] == 1:
            m_group2.append(m_tensor[i])

    # Calculate the mean for each group
    mean_group1 = torch.stack(m_group1).mean(dim=0) if m_group1 else torch.zeros(m_tensor.shape[1])
    mean_group2 = torch.stack(m_group2).mean(dim=0) if m_group2 else torch.zeros(m_tensor.shape[1])

    # Detach and convert to numpy()
    mean_group1_np = mean_group1.detach().numpy()
    mean_group2_np = mean_group2.detach().numpy()

    # Plot the mean values as a bar graph (8*2)
    fig, ax = plt.subplots()
    bar_width = 0.35
    x_indices = np.arange(mean_group1_np.shape[0])

    ax.bar(x_indices, mean_group1_np, bar_width, label='[0,0,0,0,1,1,1,1]')
    ax.bar(x_indices + bar_width, mean_group2_np, bar_width, label='[1,1,1,1,0,0,0,0]')

    ax.set_xlabel('Element Index')
    ax.set_ylabel('Mean Value')
    ax.set_title(title)
    ax.legend()

    plt.show()

# Plot mean values (training data)
plot_mean_values(obs_epoch0, avg_firing_rates_epoch0, title='Mean Values by Group (Epoch 0 - Training)')
plot_mean_values(obs_last_epoch, avg_firing_rates_last_epoch, title='Mean Values by Group (Last Epoch - Training)')

# Plot mean values (test data)
plot_mean_values(test_obs_epoch0, test_avg_firing_rates_epoch0, title='Mean Values by Group (Epoch 0 - Test)')
plot_mean_values(test_obs_last_epoch, test_avg_firing_rates_last_epoch, title='Mean Values by Group (Last Epoch - Test)')


"""Visualization of Output

# Network simulation to implement dynamic Bayesian inference
"""

# Definition of neuron populations
L4E  = torch.zeros((num_neurons, time_steps))  # L4E receives Poisson spike inputs
L2IT = torch.zeros((num_neurons, time_steps))
L3IT = torch.zeros((num_neurons, time_steps), dtype=torch.float32)  # L3IT is randomly initialized
L5IT = torch.zeros((num_neurons, time_steps))
L5PT = torch.zeros((num_neurons, time_steps))
L6IT = torch.zeros((num_neurons, time_steps))
L5PT_Inh = torch.zeros((num_neurons, time_steps))  # L5PT inhibitory neuron population
L6IT_Inh = torch.zeros((num_neurons, time_steps))  # L6IT inhibitory neuron population

# Initialize membrane potentials for each neuron population
V_L4E  = torch.zeros(num_neurons)
V_L2IT = torch.zeros(num_neurons)
V_L3IT = torch.zeros(num_neurons)
V_L5IT = torch.zeros(num_neurons)
V_L5PT = torch.zeros(num_neurons)
V_L6IT = torch.zeros(num_neurons)
V_L5PT_Inh = torch.zeros(num_neurons)  # Membrane potential of L5PT inhibitory neurons
V_L6IT_Inh = torch.zeros(num_neurons)  # Membrane potential of L6IT inhibitory neurons

# Lists to record spikes for each neuron (records spikes of each neuron)
spikes_L4E  = [[] for _ in range(num_neurons)]
spikes_L2IT = [[] for _ in range(num_neurons)]
spikes_L3IT = [[] for _ in range(num_neurons)]
spikes_L5IT = [[] for _ in range(num_neurons)]
spikes_L5PT = [[] for _ in range(num_neurons)]
spikes_L6IT = [[] for _ in range(num_neurons)]
spikes_L5PT_Inh = [[] for _ in range(num_neurons)]  # Spikes of L5PT inhibitory neurons
spikes_L6IT_Inh = [[] for _ in range(num_neurons)]  # Spikes of L6IT inhibitory neurons

# Store spikes from the previous time step for inhibitory neurons
spike_L5PT_Inh_prev = torch.zeros(num_neurons)
spike_L6IT_Inh_prev = torch.zeros(num_neurons)

# Generate observation data
obs = torch.tensor(yt, dtype=torch.long)

# Generate activity for L4E population using SNN
input_spikes_sim = generate_input_spikes(obs, duration_steps, dt)

# Randomly initialize L3IT's initial state
prev_L5PT_spikes = torch.randint(0, 2, (1, duration_steps, num_neurons))

# Transmission and computation (using LIF model)
for t in range(time_steps):

    # obs -> L4E
    output_L4E = snn(input_spikes_sim[t:t+1,:,:] * w[2], W1, W2)
    spike_L4E = output_L4E.squeeze(0).squeeze(0).bool()  # Size: [num_neurons]
    L4E[:, t] = spike_L4E.float()
    for neuron_idx in spike_L4E.nonzero(as_tuple=True)[0]:
        spikes_L4E[neuron_idx].append(t * dt)

    # L4E -> L2IT
    background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[0]))
    # Retrieve spike information of spikes_L4E at time t and convert to tensor
    spike_L4E_at_t = torch.tensor([1 if t in spikes else 0 for spikes in spikes_L4E])
    I_L2IT = spike_L4E_at_t * w[0] + background_spikes  # Input current based on synaptic connections
    V_L2IT, spike_L2IT = lif_update(V_L2IT, I_L2IT, dt, tau_m, V_reset, V_threshold)
    L2IT[:, t] = V_L2IT  # Membrane potential of L2IT
    for neuron_idx in spike_L2IT.nonzero(as_tuple=True)[0]:
        spikes_L2IT[neuron_idx].append(t * dt)  # Record firing neuron index and time

    # Prepare input for L3IT (previous output)
    input_L3IT = prev_L5PT_spikes

    # Execute network for L3IT
    output_L3IT = snn_transition(input_L3IT.float() * w[1], W1_L3IT, W2_L3IT)
    spike_L3IT = output_L3IT.squeeze(0).squeeze(0).bool()  # Size: [num_neurons]
    L3IT[:, t] = spike_L3IT.float()
    for neuron_idx in spike_L3IT.nonzero(as_tuple=True)[0]:
        spikes_L3IT[neuron_idx].append(t * dt)

    # L3IT -> (combined with L5PT output) -> L5IT
    background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[1]))
    I_L5IT = spike_L3IT * w[3] + background_spikes
    V_L5IT, spike_L5IT = lif_update(V_L5IT, I_L5IT, dt, tau_m, V_reset, V_threshold)
    L5IT[:, t] = V_L5IT  # Membrane potential of L5IT
    for neuron_idx in spike_L5IT.nonzero(as_tuple=True)[0]:
        spikes_L5IT[neuron_idx].append(t * dt)

    # L5IT -> (combined with L2IT output) -> L5PT
    background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[2]))
    I_L5PT = spike_L5IT * w[4] * spike_L2IT + background_spikes - spike_L5PT_Inh_prev * w_Inh_to_L5PT
    V_L5PT, spike_L5PT = lif_update(V_L5PT, I_L5PT, dt, tau_m, V_reset, V_threshold)
    L5PT[:, t] = V_L5PT  # Membrane potential of L5PT
    for neuron_idx in spike_L5PT.nonzero(as_tuple=True)[0]:
        spikes_L5PT[neuron_idx].append(t * dt)

    # Input and update for inhibitory neurons from L5PT
    background_spikes_inh = torch.bernoulli(torch.full((num_neurons,), background_rate[4]))
    I_L5PT_Inh = spike_L5PT * w_L5PT_to_Inh + background_spikes_inh
    V_L5PT_Inh, spike_L5PT_Inh = lif_update(V_L5PT_Inh, I_L5PT_Inh, dt, tau_m, V_reset, V_threshold)
    L5PT_Inh[:, t] = V_L5PT_Inh  # Membrane potential of L5PT inhibitory neurons
    for neuron_idx in spike_L5PT_Inh.nonzero(as_tuple=True)[0]:
        spikes_L5PT_Inh[neuron_idx].append(t * dt)

    # Save spikes of inhibitory neurons for the next time step
    spike_L5PT_Inh_prev = spike_L5PT_Inh

    # L5PT -> L6IT
    background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[3]))
    I_L6IT = spike_L5PT * w[5] + background_spikes - spike_L6IT_Inh_prev * w_Inh_to_L6IT
    V_L6IT, spike_L6IT = lif_update(V_L6IT, I_L6IT, dt, tau_m, V_reset, V_threshold)
    L6IT[:, t] = V_L6IT  # Membrane potential of L6IT
    for neuron_idx in spike_L6IT.nonzero(as_tuple=True)[0]:
        spikes_L6IT[neuron_idx].append(t * dt)

    # Calculate the average activity of L6IT
    mean_activity_L6IT = spike_L6IT.float().mean()

    # Input and update for L6IT inhibitory neurons
    background_spikes_inh_L6IT = torch.bernoulli(torch.full((num_neurons,), background_rate[5]))
    I_L6IT_Inh = spike_L6IT * w_L6IT_to_Inh + background_spikes_inh_L6IT
    V_L6IT_Inh, spike_L6IT_Inh = lif_update(V_L6IT_Inh, I_L6IT_Inh, dt, tau_m, V_reset, V_threshold)
    L6IT_Inh[:, t] = V_L6IT_Inh  # Membrane potential of L6IT inhibitory neurons
    for neuron_idx in spike_L6IT_Inh.nonzero(as_tuple=True)[0]:
        spikes_L6IT_Inh[neuron_idx].append(t * dt)

    # Save L5PT output for the next time step
    prev_L5PT_spikes = spike_L5PT.unsqueeze(0).unsqueeze(0).float()  # Size: [1, num_neurons, 1]
    # Save spikes of inhibitory neurons for the next time step
    spike_L6IT_Inh_prev = spike_L6IT_Inh


# Raster plot of spike time series data
def plot_raster(spikes, title):
    plt.eventplot(spikes, colors='black', lineoffsets=1, linelengths=0.8)
    plt.title(title)
    plt.xlabel('Time (ms)')
    plt.ylabel('Neuron index')

# Plot spike times for each neuron population
plt.figure(figsize=(12, 12))

plt.subplot(5, 2, 1)
plot_raster(spikes_L4E, "L4E Spikes")

plt.subplot(5, 2, 2)
plot_raster(spikes_L2IT, "L2IT Spikes")

plt.subplot(5, 2, 3)
plot_raster(spikes_L3IT, "L3IT Spikes")

plt.subplot(5, 2, 4)
plot_raster(spikes_L5IT, "L5IT Spikes")

plt.subplot(5, 2, 5)
plot_raster(spikes_L5PT, "L5PT Spikes")

plt.subplot(5, 2, 6)
plot_raster(spikes_L6IT, "L6IT Spikes")

plt.subplot(5, 2, 7)
plot_raster(spikes_L5PT_Inh, "L5PT Inhibitory Spikes")

plt.subplot(5, 2, 8)
plot_raster(spikes_L6IT_Inh, "L6IT Inhibitory Spikes")

plt.tight_layout()
plt.show()

# Combine spike activity of all neuron populations vertically and display raster plot with specified order and colors

# List of spike data, colors, and labels for neuron populations
neuron_data = [
    (spikes_L6IT_Inh, 'black', 'L6SST'),   # Name changed
    (spikes_L6IT, 'cyan', 'L6IT'),
    (spikes_L5PT_Inh, 'black', 'L5SST'),   # Name changed
    (spikes_L5PT, 'blue', 'L5PT'),
    (spikes_L5IT, 'orange', 'L5IT'),
    (spikes_L4E, 'red', 'L4E'),
    (spikes_L3IT, 'green', 'L3IT'),
    (spikes_L2IT, 'red', 'L2IT')
]

# Get the number of neurons for each population
neuron_counts = [len(spikes) for spikes, _, _ in neuron_data]
# Compute the cumulative sum of neuron counts
cumulative_counts = np.cumsum([0] + neuron_counts)

# Prepare for plotting
plt.figure(figsize=(3, 5))

positions = []  # List to store positions of all neurons

for i, (spikes, color, label) in enumerate(neuron_data):
    # Compute offset
    offset = cumulative_counts[i]
    # Get spike times for each neuron
    adjusted_spikes = [s for s in spikes]
    # Apply offset and adjust neuron index
    adjusted_spikes_with_offset = []
    for neuron_index, neuron_spikes in enumerate(adjusted_spikes):
        adjusted_neuron_index = neuron_index + offset
        adjusted_spikes_with_offset.append((adjusted_neuron_index, neuron_spikes))
        positions.append(adjusted_neuron_index)  # Save position
    # Get spike times and positions
    spike_times = [neuron_spikes for adjusted_neuron_index, neuron_spikes in adjusted_spikes_with_offset]
    neuron_positions = [adjusted_neuron_index for adjusted_neuron_index, neuron_spikes in adjusted_spikes_with_offset]
    # Create color list
    line_colors = [color] * len(neuron_positions)
    # Plot spikes
    plt.eventplot(spike_times, lineoffsets=neuron_positions, colors=line_colors, linelengths=0.8)

# Calculate label positions
label_positions = []
for i in range(len(neuron_counts)):
    start = cumulative_counts[i]
    end = cumulative_counts[i+1]
    center = (start + end - 1) / 2
    label_positions.append(center)

# Set y-axis labels
labels = [label for _, _, label in neuron_data]
plt.yticks(label_positions, labels)

# Adjust y-axis range to remove extra space at top and bottom
min_position = min(positions)
max_position = max(positions)
plt.ylim(min_position - 0.5, max_position + 0.5)

plt.xlabel('Time (ms)')
plt.ylabel('Neuron Populations')
plt.title('Raster Plot of Neuron Populations')

plt.tight_layout()
plt.show()


# Function to calculate the average firing rate for a specified state
def calculate_average_firing_rate_for_state(spikes, time_steps, dt, ns=4, state_index=0):
    """
    Calculates the average firing rate for the specified state.

    spikes: List of spike records for each neuron
    time_steps: Total number of time steps
    dt: Time step duration
    ns: Number of neurons per state (default is 4)
    state_index: Index of the state for which to calculate the firing rate (default is 0)
    """

    # Calculate the index range for neurons corresponding to the state
    start_idx = state_index * ns
    end_idx = (state_index + 1) * ns
    state_neurons_spikes = spikes[start_idx:end_idx]

    # Calculate the firing rate for neurons corresponding to the specified state
    average_firing_rates = torch.zeros(time_steps)

    for t in range(time_steps):
        # Count the number of neurons that fired in the specified state
        spike_count = sum([1 for neuron_spikes in state_neurons_spikes if t * dt in neuron_spikes])
        average_firing_rates[t] = spike_count / ns  # Divide by the number of neurons in the state

    return average_firing_rates


# Calculate the average firing rate for each population
firing_rate_L4E  = calculate_average_firing_rate_for_state(spikes_L4E, time_steps, dt)
firing_rate_L2IT = calculate_average_firing_rate_for_state(spikes_L2IT, time_steps, dt)
firing_rate_L3IT = calculate_average_firing_rate_for_state(spikes_L3IT, time_steps, dt)
firing_rate_L5IT = calculate_average_firing_rate_for_state(spikes_L5IT, time_steps, dt)
firing_rate_L5PT = calculate_average_firing_rate_for_state(spikes_L5PT, time_steps, dt)
firing_rate_L6IT = calculate_average_firing_rate_for_state(spikes_L6IT, time_steps, dt)

# Create a new figure for the line plots
fig, ax = plt.subplots(figsize=(10, 6))

# Plot L2IT firing rate as a line plot
ax.plot(firing_rate_L2IT, color='red', label='L2IT', linewidth=1.5)

# Plot L3IT firing rate as a line plot
ax.plot(firing_rate_L3IT, color='green', label='L3IT', linewidth=1.5)

# Plot L4E firing rate as a line plot
ax.plot(firing_rate_L4E, color='red', label='L4E', linewidth=1.5)

# Plot L5IT firing rate as a line plot
ax.plot(firing_rate_L5IT, color='orange', label='L5IT', linewidth=1.5)

# Plot L5PT firing rate as a line plot
ax.plot(firing_rate_L5PT, color='blue', label='L5PT', linewidth=1.5)

# Plot L6IT firing rate as a line plot
ax.plot(firing_rate_L6IT, color='cyan', label='L6IT', linewidth=1.5)

# Add titles and labels
ax.set_title('Line Plots of All Variables', fontsize=16)
ax.set_xlabel('t', fontsize=12)
ax.set_ylabel('Value', fontsize=12)

# Add a legend to differentiate the plots
ax.legend(loc='upper right')

# Add grid for better readability
ax.grid(True)

# Show the plot
plt.tight_layout()
plt.show()

# Create separate figures for each plot
fig, ax = plt.subplots(6, 1, figsize=(10, 18))  # Create 6 subplots

# Plot L2IT firing rate
ax[0].plot(firing_rate_L2IT, color='red', linewidth=1.5)
ax[0].set_title('L2IT', fontsize=32)
ax[0].set_xlabel('t', fontsize=24)
ax[0].set_ylabel('Value', fontsize=24)
ax[0].grid(True)

# Plot L3IT firing rate
ax[1].plot(firing_rate_L3IT, color='green', linewidth=1.5)
ax[1].set_title('L3IT', fontsize=32)
ax[1].set_xlabel('t', fontsize=24)
ax[1].set_ylabel('Value', fontsize=24)
ax[1].grid(True)

# Plot L4E firing rate
ax[2].plot(firing_rate_L4E, color='red', linewidth=1.5)
ax[2].set_title('L4E', fontsize=32)
ax[2].set_xlabel('t', fontsize=24)
ax[2].set_ylabel('Value', fontsize=24)
ax[2].grid(True)

# Plot L5IT firing rate
ax[3].plot(firing_rate_L5IT, color='orange', linewidth=1.5)
ax[3].set_title('L5IT', fontsize=32)
ax[3].set_xlabel('t', fontsize=24)
ax[3].set_ylabel('Value', fontsize=24)
ax[3].grid(True)

# Plot L5PT firing rate
ax[4].plot(firing_rate_L5PT, color='blue', linewidth=1.5)
ax[4].set_title('L5PT', fontsize=32)
ax[4].set_xlabel('t', fontsize=24)
ax[4].set_ylabel('Value', fontsize=24)
ax[4].grid(True)

# Plot L6IT firing rate
ax[5].plot(firing_rate_L6IT, color='cyan', linewidth=1.5)
ax[5].set_title('L6IT', fontsize=32)
ax[5].set_xlabel('t', fontsize=24)
ax[5].set_ylabel('Value', fontsize=24)
ax[5].grid(True)

# Adjust layout to prevent overlapping
plt.tight_layout()

# Show the plot
plt.show()

# Create matrices for evaluation metrics
matrix_6 = np.vstack([likelihood[:,0], prediction_array[:,0], likelihood[:,0], predictiveprior[:,0], post[:,0], mapestimate[:,0], xt]).T
Ex_act = np.vstack([
    firing_rate_L2IT,
    firing_rate_L3IT,
    firing_rate_L4E,
    firing_rate_L5IT,
    firing_rate_L5PT,
    firing_rate_L6IT
]).T

# Standardize each column of the matrices
matrix_6 = (matrix_6 - np.mean(matrix_6, axis=0)) / np.std(matrix_6, axis=0)
Ex_act = (Ex_act - np.mean(Ex_act, axis=0)) / np.std(Ex_act, axis=0)

# Initialize matrices to store results
mse_matrix = np.zeros((6, 6))
r2_matrix = np.zeros((6, 6))
pearson_matrix = np.zeros((6, 6))

# Calculate metrics for each pair
for i in range(6):
    for j in range(6):
        mse_matrix[i, j] = mean_squared_error(matrix_6[:, i], Ex_act[:, j])
        r2_matrix[i, j] = r2_score(matrix_6[:, i], Ex_act[:, j])
        pearson_matrix[i, j], _ = pearsonr(matrix_6[:, i], Ex_act[:, j])

# Custom labels
x_labels = ['L2IT', 'L3IT', 'L4E', 'L5IT', 'L5PT', 'L6IT']
y_labels = ['Likelihood', 'Prediction', 'Likelihood', 'Predictive Prior', 'Posterior', 'MAP']

# Plot heatmaps with adjusted font size
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# MSE heatmap
sns.heatmap(mse_matrix, ax=axes[0], cmap='coolwarm_r', annot=True, annot_kws={"size": 16})
axes[0].set_xticklabels(x_labels, rotation=45, fontsize=20)
axes[0].set_yticklabels(y_labels, rotation=0, fontsize=20)

# R-Squared heatmap
sns.heatmap(r2_matrix, ax=axes[1], cmap='coolwarm', annot=True, annot_kws={"size": 16})
axes[1].set_xticklabels(x_labels, rotation=45, fontsize=20)
axes[1].set_yticklabels(y_labels, rotation=0, fontsize=20)

# Pearson Correlation heatmap
sns.heatmap(pearson_matrix, ax=axes[2], cmap='coolwarm', annot=True, annot_kws={"size": 16})
axes[2].set_xticklabels(x_labels, rotation=45, fontsize=20)
axes[2].set_yticklabels(y_labels, rotation=0, fontsize=20)

plt.tight_layout()
plt.show()


"""# Repeated Execution of Dynamic Bayesian Inference for Quantifying Results"""

num_repeats = 10

# Prepare lists to store the results
xt_results = []
yt_results = []
likelihood_results = []
prediction_array_results = []
predictiveprior_results = []
post_results = []
mapestimate_results = []
start_times_results = []  # Added
durations_results = []    # Added

# Perform repeated execution
for iteration in range(num_repeats):
    # Call the function to obtain the start times and durations of observation 1
    start_times, durations, xt, yt = ring.simulate_observations(T)

    # Dynamic Bayesian inference in HMM
    post = np.zeros((T, ns))  # posterior trajectory
    observation = np.zeros((T, no))
    likelihood = np.zeros((T, ns))
    prediction = np.zeros((ns, ns))
    predictiveprior = np.zeros((T, ns))
    mapestimate = np.zeros((T, ns))

    ring.reset()
    for t in range(T):
        post[t], observation[t], likelihood[t], prediction, predictiveprior[t], mapestimate[t] = ring.step(yt[t])

    # Store the results
    xt_results.append(xt)
    yt_results.append(yt)
    likelihood_results.append(likelihood)
    prediction_array_results.append(predictiveprior)
    predictiveprior_results.append(predictiveprior)
    post_results.append(post)
    mapestimate_results.append(mapestimate)
    start_times_results.append(start_times)
    durations_results.append(durations)

# Save the results as a DataFrame
results_df = pd.DataFrame({
    'xt': xt_results,
    'yt': yt_results,
    'likelihood': likelihood_results,
    'prediction_array': prediction_array_results,
    'predictiveprior': predictiveprior_results,
    'post': post_results,
    'mapestimate': mapestimate_results,
    'start_times': start_times_results,
    'durations': durations_results
})

# Save the results to a CSV file
results_df.to_csv('hmm_results.csv', index=False)

print("Results have been saved to 'hmm_results.csv'.")


"""# Repeated Execution of SNN Simulation for Quantifying Results"""

# Prepare lists to store the results
firing_rate_L4E_results = []
firing_rate_L2IT_results = []
firing_rate_L3IT_results = []
firing_rate_L5IT_results = []
firing_rate_L5PT_results = []
firing_rate_L6IT_results = []

# Repeat the simulation
for iteration in range(num_repeats):
    if iteration % 20 == 0:
        print(f"Iteration {iteration + 1} started")

    # Define neuron populations and initialize membrane potentials
    # Define populations such as L4E, L2IT, L3IT, L5IT, L5PT, and L6IT
    L4E = torch.zeros((num_neurons, time_steps))
    L2IT = torch.zeros((num_neurons, time_steps))
    L3IT = torch.zeros((num_neurons, time_steps))
    L5IT = torch.zeros((num_neurons, time_steps))
    L5PT = torch.zeros((num_neurons, time_steps))
    L6IT = torch.zeros((num_neurons, time_steps))
    L5PT_Inh = torch.zeros((num_neurons, time_steps))  # L5PT inhibitory neuron population
    L6IT_Inh = torch.zeros((num_neurons, time_steps))  # L6IT inhibitory neuron population

    # Initialize membrane potentials for each neuron population
    V_L4E  = torch.zeros(num_neurons)
    V_L2IT = torch.zeros(num_neurons)
    V_L3IT = torch.zeros(num_neurons)
    V_L5IT = torch.zeros(num_neurons)
    V_L5PT = torch.zeros(num_neurons)
    V_L6IT = torch.zeros(num_neurons)
    V_L5PT_Inh = torch.zeros(num_neurons)  # Membrane potential for L5PT inhibitory neurons
    V_L6IT_Inh = torch.zeros(num_neurons)  # Membrane potential for L6IT inhibitory neurons

    # Lists to record spikes
    spikes_L4E = [[] for _ in range(num_neurons)]
    spikes_L2IT = [[] for _ in range(num_neurons)]
    spikes_L3IT = [[] for _ in range(num_neurons)]
    spikes_L5IT = [[] for _ in range(num_neurons)]
    spikes_L5PT = [[] for _ in range(num_neurons)]
    spikes_L6IT = [[] for _ in range(num_neurons)]
    spikes_L5PT_Inh = [[] for _ in range(num_neurons)]  # Spikes for L5PT inhibitory neurons
    spikes_L6IT_Inh = [[] for _ in range(num_neurons)]  # Spikes for L6IT inhibitory neurons

    # Store inhibitory neuron spikes from the previous time step
    spike_L5PT_Inh_prev = torch.zeros(num_neurons)
    spike_L6IT_Inh_prev = torch.zeros(num_neurons)

    # Generate observation data
    obs = torch.tensor(yt_results[iteration], dtype=torch.long)

    # Generate activity for the L4E population using the SNN
    input_spikes_sim = generate_input_spikes(obs, duration_steps, dt)

    # Randomly initialize the initial state of L3IT
    prev_L5PT_spikes = torch.randint(0, 2, (1, duration_steps, num_neurons))

    # Update the membrane potentials and spikes of each neuron population in the simulation
    # Transmission and computation (using the LIF model)
    for t in range(time_steps):

        # obs -> L4E
        output_L4E = snn(input_spikes_sim[t:t+1,:,:] * w[2], W1, W2)
        spike_L4E = output_L4E.squeeze(0).squeeze(0).bool()  # Size: [num_neurons]
        L4E[:, t] = spike_L4E.float()
        for neuron_idx in spike_L4E.nonzero(as_tuple=True)[0]:
            spikes_L4E[neuron_idx].append(t * dt)

        # L4E -> L2IT
        background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[0]))
        # Get the spike information of spikes_L4E at time t and convert it to a tensor
        spike_L4E_at_t = torch.tensor([1 if t in spikes else 0 for spikes in spikes_L4E])
        I_L2IT = spike_L4E_at_t * w[0] + background_spikes  # Input current based on synaptic connections
        V_L2IT, spike_L2IT = lif_update(V_L2IT, I_L2IT, dt, tau_m, V_reset, V_threshold)
        L2IT[:, t] = V_L2IT  # Membrane potential of L2IT
        for neuron_idx in spike_L2IT.nonzero(as_tuple=True)[0]:
            spikes_L2IT[neuron_idx].append(t * dt)  # Record the index and time of the firing neuron

        # Prepare input for L3IT (previous output)
        input_L3IT = prev_L5PT_spikes

        # Execute the network for L3IT
        output_L3IT = snn_transition(input_L3IT.float() * w[1], W1_L3IT, W2_L3IT)
        spike_L3IT = output_L3IT.squeeze(0).squeeze(0).bool()  # Size: [num_neurons]
        L3IT[:, t] = spike_L3IT.float()
        for neuron_idx in spike_L3IT.nonzero(as_tuple=True)[0]:
            spikes_L3IT[neuron_idx].append(t * dt)

        # L3IT -> (combined with L5PT output) -> L5IT
        background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[1]))
        # Obtain output
        I_L5IT = spike_L3IT * w[3] + background_spikes
        V_L5IT, spike_L5IT = lif_update(V_L5IT, I_L5IT, dt, tau_m, V_reset, V_threshold)
        L5IT[:, t] = V_L5IT  # Membrane potential of L5IT
        for neuron_idx in spike_L5IT.nonzero(as_tuple=True)[0]:
            spikes_L5IT[neuron_idx].append(t * dt)

        # L5IT -> (combined with L2IT output) -> L5PT
        background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[2]))
        I_L5PT = spike_L5IT * w[4] * spike_L2IT + background_spikes - spike_L5PT_Inh_prev * w_Inh_to_L5PT
        V_L5PT, spike_L5PT = lif_update(V_L5PT, I_L5PT, dt, tau_m, V_reset, V_threshold)
        L5PT[:, t] = V_L5PT  # Membrane potential of L5PT
        for neuron_idx in spike_L5PT.nonzero(as_tuple=True)[0]:
            spikes_L5PT[neuron_idx].append(t * dt)

        # Input and update for inhibitory neurons from L5PT
        background_spikes_inh = torch.bernoulli(torch.full((num_neurons,), background_rate[4]))
        I_L5PT_Inh = spike_L5PT * w_L5PT_to_Inh + background_spikes_inh
        V_L5PT_Inh, spike_L5PT_Inh = lif_update(V_L5PT_Inh, I_L5PT_Inh, dt, tau_m, V_reset, V_threshold)
        L5PT_Inh[:, t] = V_L5PT_Inh  # Membrane potential of L5PT inhibitory neurons
        for neuron_idx in spike_L5PT_Inh.nonzero(as_tuple=True)[0]:
            spikes_L5PT_Inh[neuron_idx].append(t * dt)

        # Save inhibitory neuron spikes for the next time step
        spike_L5PT_Inh_prev = spike_L5PT_Inh

        # L5PT -> L6IT
        background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[3]))
        I_L6IT = spike_L5PT * w[5] + background_spikes - spike_L6IT_Inh_prev * w_Inh_to_L6IT
        V_L6IT, spike_L6IT = lif_update(V_L6IT, I_L6IT, dt, tau_m, V_reset, V_threshold)
        L6IT[:, t] = V_L6IT  # Membrane potential of L6IT
        for neuron_idx in spike_L6IT.nonzero(as_tuple=True)[0]:
            spikes_L6IT[neuron_idx].append(t * dt)

        # Calculate average activity of L6IT
        mean_activity_L6IT = spike_L6IT.float().mean()

        # Input and update for L6IT inhibitory neurons
        background_spikes_inh_L6IT = torch.bernoulli(torch.full((num_neurons,), background_rate[5]))
        I_L6IT_Inh = spike_L6IT * w_L6IT_to_Inh + background_spikes_inh_L6IT
        V_L6IT_Inh, spike_L6IT_Inh = lif_update(V_L6IT_Inh, I_L6IT_Inh, dt, tau_m, V_reset, V_threshold)
        L6IT_Inh[:, t] = V_L6IT_Inh  # Membrane potential of L6IT inhibitory neurons
        for neuron_idx in spike_L6IT_Inh.nonzero(as_tuple=True)[0]:
            spikes_L6IT_Inh[neuron_idx].append(t * dt)

        # Save the output of L5PT for the next time step
        prev_L5PT_spikes = spike_L5PT.unsqueeze(0).unsqueeze(0).float()  # Size: [1, num_neurons, 1]
        # Save inhibitory neuron spikes for the next time step
        spike_L6IT_Inh_prev = spike_L6IT_Inh

    # Calculate the average firing rate for each population
    firing_rate_L4E  = calculate_average_firing_rate_for_state(spikes_L4E, time_steps, dt)
    firing_rate_L2IT = calculate_average_firing_rate_for_state(spikes_L2IT, time_steps, dt)
    firing_rate_L3IT = calculate_average_firing_rate_for_state(spikes_L3IT, time_steps, dt)
    firing_rate_L5IT = calculate_average_firing_rate_for_state(spikes_L5IT, time_steps, dt)
    firing_rate_L5PT = calculate_average_firing_rate_for_state(spikes_L5PT, time_steps, dt)
    firing_rate_L6IT = calculate_average_firing_rate_for_state(spikes_L6IT, time_steps, dt)

    # Append the results to the list
    firing_rate_L4E_results.append(firing_rate_L4E.numpy())
    firing_rate_L2IT_results.append(firing_rate_L2IT.numpy())
    firing_rate_L3IT_results.append(firing_rate_L3IT.numpy())
    firing_rate_L5IT_results.append(firing_rate_L5IT.numpy())
    firing_rate_L5PT_results.append(firing_rate_L5PT.numpy())
    firing_rate_L6IT_results.append(firing_rate_L6IT.numpy())


"""# Plotting the Average of Repeated Simulation Results"""

# Prepare lists to store the results
mse_list = []
r2_list = []
pearson_list = []

# Repeat the simulation
for iteration in range(num_repeats):
    # Generate matrix_6 and Ex_act based on the original logic
    matrix_6 = np.vstack([likelihood_results[iteration][:, 0], prediction_array_results[iteration][:, 0], likelihood_results[iteration][:, 0], predictiveprior_results[iteration][:, 0], post_results[iteration][:, 0], mapestimate_results[iteration][:, 0]]).T
    Ex_act = np.vstack([firing_rate_L2IT_results[iteration], firing_rate_L3IT_results[iteration], firing_rate_L4E_results[iteration], firing_rate_L5IT_results[iteration], firing_rate_L5PT_results[iteration], firing_rate_L6IT_results[iteration]]).T

    # Standardize each column
    matrix_6 = (matrix_6 - np.mean(matrix_6, axis=0)) / np.std(matrix_6, axis=0)
    Ex_act = (Ex_act - np.mean(Ex_act, axis=0)) / np.std(Ex_act, axis=0)

    # Initialize matrices to store results for each iteration
    mse_matrix = np.zeros((6, 6))
    r2_matrix = np.zeros((6, 6))
    pearson_matrix = np.zeros((6, 6))

    # Calculate metrics for each combination
    for i in range(6):
        for j in range(6):
            mse_matrix[i, j] = mean_squared_error(matrix_6[:, i], Ex_act[:, j])
            r2_matrix[i, j] = r2_score(matrix_6[:, i], Ex_act[:, j])
            pearson_matrix[i, j], _ = pearsonr(matrix_6[:, i], Ex_act[:, j])

    # Append the results of each iteration to the list
    mse_list.append(mse_matrix)
    r2_list.append(r2_matrix)
    pearson_list.append(pearson_matrix)

# Calculate the average of each metric
mse_avg = np.mean(mse_list, axis=0).T
r2_avg = np.mean(r2_list, axis=0).T
pearson_avg = np.mean(pearson_list, axis=0).T

# Custom labels
y_labels = ['L2IT', 'L3IT', 'L4E', 'L5IT', 'L5PT', 'L6IT']
x_labels = ['Likelihood', 'Prediction', 'Likelihood', 'Predictive Prior', 'Posterior', 'MAP']

# Create the plots
fig, axes = plt.subplots(1, 3, figsize=(16, 7))

# MSE heatmap
sns.heatmap(mse_avg, ax=axes[0], cmap='coolwarm_r', annot=True, annot_kws={"size": 16}, vmin=0, vmax=1.6)
axes[0].set_xticklabels(x_labels, rotation=90, fontsize=20)
axes[0].set_yticklabels(y_labels, rotation=0, fontsize=20)
axes[0].set_title('Mean Squared Error (MSE)', fontsize=20)

# R-Squared heatmap
sns.heatmap(r2_avg, ax=axes[1], cmap='coolwarm', annot=True, annot_kws={"size": 16}, vmin=0, vmax=1)
axes[1].set_xticklabels(x_labels, rotation=90, fontsize=20)
axes[1].set_yticklabels(y_labels, rotation=0, fontsize=20)
axes[1].set_title('R-Squared (Explained Variance)', fontsize=20)

# Pearson Correlation heatmap
sns.heatmap(pearson_avg, ax=axes[2], cmap='coolwarm', annot=True, annot_kws={"size": 16}, vmin=0, vmax=1)
axes[2].set_xticklabels(x_labels, rotation=90, fontsize=20)
axes[2].set_yticklabels(y_labels, rotation=0, fontsize=20)
axes[2].set_title('Pearson Correlation', fontsize=20)

plt.tight_layout()
plt.show()


"""# Hyperparameter Optimization using Optuna"""

# !pip install optuna
import optuna

def objective(trial):

    # Suggest background firing rates (br0 - br5) for each neuronal population
    br0 = trial.suggest_uniform('br0', 0, 1)
    br1 = trial.suggest_uniform('br1', 0, 1)
    br2 = trial.suggest_uniform('br2', 0, 1)
    br3 = trial.suggest_uniform('br3', 0, 1)
    br4 = trial.suggest_uniform('br4', 0, 1)
    br5 = trial.suggest_uniform('br5', 0, 1)

    # Parameter setting section
    # Define the number of neurons (as 2^n)
    n = 7  # Example: n=3 (the size of the neuron population is 2^n)
    num_neurons = 2**n

    # Number of simulation time steps
    time_steps = 100
    dt = 1  # Time step (ms)

    # LIF model parameters
    tau_m = 20  # Time constant (ms)
    V_reset = 0.0  # Reset voltage
    V_threshold = 0.5  # Firing threshold
    w = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # Synaptic weights: L4->L2IT, prevL5PT->L3IT, obs->L4E, L3IT->L5IT, L2IT*L5IT->L5PT, L5PT->L6IT
    background_rate = [br0, br1, br2, br3, br4, br5]  # Background firing rates: L2IT, L5IT, L5PT, L6IT, L5SST, L6SST
    beta = 12  # Steepness of the spike function

    # Synaptic weights for inhibitory neurons
    w_L5PT_to_Inh = 0.80  # Weight from L5PT to inhibitory neurons
    w_Inh_to_L5PT = 0.17  # Inhibition from inhibitory neurons to L5PT
    w_L6IT_to_Inh = 1.5    # Weight from L6IT to inhibitory neurons (if needed)
    w_Inh_to_L6IT = 0.18   # Inhibition from inhibitory neurons to L6IT

    # Generate training data
    num_samples = 1000

    # Simulation parameters
    duration = 1   # ms
    duration_steps = int(duration / dt)

    # Training parameters
    input_size = no
    num_hidden = num_neurons
    nb_epochs = 10
    lr = 0.01
    batch_size = 32

    # Dynamic Bayesian Inference section
    # Prepare lists to store results
    xt_results = []
    yt_results = []
    likelihood_results = []
    prediction_array_results = []
    predictiveprior_results = []
    post_results = []
    mapestimate_results = []

    # Repeat the simulation 10 times
    for iteration in range(10):
        # Generate start time and duration for observation 1
        start_times, durations, xt, yt = ring.simulate_observations(T)

        # Dynamic Bayesian inference in HMM
        post = np.zeros((T, ns))  # posterior trajectory
        observation = np.zeros((T, no))
        likelihood = np.zeros((T, ns))
        prediction = np.zeros((ns, ns))
        predictiveprior = np.zeros((T, ns))
        mapestimate = np.zeros((T, ns))

        ring.reset()
        for t in range(T):
            post[t], observation[t], likelihood[t], prediction, predictiveprior[t], mapestimate[t] = ring.step(yt[t])

        # Save results
        xt_results.append(xt)
        yt_results.append(yt)
        likelihood_results.append(likelihood)
        prediction_array_results.append(predictiveprior)
        predictiveprior_results.append(predictiveprior)
        post_results.append(post)
        mapestimate_results.append(mapestimate)

    # SNN simulation section
    # Prepare lists to store results
    firing_rate_L4E_results = []
    firing_rate_L2IT_results = []
    firing_rate_L3IT_results = []
    firing_rate_L5IT_results = []
    firing_rate_L5PT_results = []
    firing_rate_L6IT_results = []

    # Repeat the simulation 10 times
    for iteration in range(10):
        if iteration % 20 == 0:
            print(f"Starting iteration {iteration + 1}")

        # Initialize neuron populations and membrane potentials
        # Define the populations: L4E, L2IT, L3IT, L5IT, L5PT, L6IT
        L4E = torch.zeros((num_neurons, time_steps))
        L2IT = torch.zeros((num_neurons, time_steps))
        L3IT = torch.zeros((num_neurons, time_steps))
        L5IT = torch.zeros((num_neurons, time_steps))
        L5PT = torch.zeros((num_neurons, time_steps))
        L6IT = torch.zeros((num_neurons, time_steps))
        L5PT_Inh = torch.zeros((num_neurons, time_steps))  # L5PT inhibitory neurons
        L6IT_Inh = torch.zeros((num_neurons, time_steps))  # L6IT inhibitory neurons

        # Initialize membrane potentials for each population
        V_L4E  = torch.zeros(num_neurons)
        V_L2IT = torch.zeros(num_neurons)
        V_L3IT = torch.zeros(num_neurons)
        V_L5IT = torch.zeros(num_neurons)
        V_L5PT = torch.zeros(num_neurons)
        V_L6IT = torch.zeros(num_neurons)
        V_L5PT_Inh = torch.zeros(num_neurons)  # Membrane potential for L5PT inhibitory neurons
        V_L6IT_Inh = torch.zeros(num_neurons)  # Membrane potential for L6IT inhibitory neurons

        # Lists to record spikes
        spikes_L4E = [[] for _ in range(num_neurons)]
        spikes_L2IT = [[] for _ in range(num_neurons)]
        spikes_L3IT = [[] for _ in range(num_neurons)]
        spikes_L5IT = [[] for _ in range(num_neurons)]
        spikes_L5PT = [[] for _ in range(num_neurons)]
        spikes_L6IT = [[] for _ in range(num_neurons)]
        spikes_L5PT_Inh = [[] for _ in range(num_neurons)]  # L5PT inhibitory neuron spikes
        spikes_L6IT_Inh = [[] for _ in range(num_neurons)]  # L6IT inhibitory neuron spikes

        # Keep the previous step's inhibitory spikes
        spike_L5PT_Inh_prev = torch.zeros(num_neurons)
        spike_L6IT_Inh_prev = torch.zeros(num_neurons)

        # Generate observation data
        obs = torch.tensor(yt_results[iteration], dtype=torch.long)

        # Generate activity in L4E using SNN
        input_spikes_sim = generate_input_spikes(obs, duration_steps, dt)

        # Randomly initialize the state of L3IT
        prev_L5PT_spikes = torch.randint(0, 2, (1, duration_steps, num_neurons))

        # Update membrane potentials and spikes for each population (using the LIF model)
        for t in range(time_steps):

            # obs -> L4E
            output_L4E = snn(input_spikes_sim[t:t+1,:,:] * w[2], W1, W2)
            spike_L4E = output_L4E.squeeze(0).squeeze(0).bool()  # Spike output of L4E neurons
            L4E[:, t] = spike_L4E.float()
            for neuron_idx in spike_L4E.nonzero(as_tuple=True)[0]:
                spikes_L4E[neuron_idx].append(t * dt)

            # L4E -> L2IT
            background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[0]))
            spike_L4E_at_t = torch.tensor([1 if t in spikes else 0 for spikes in spikes_L4E])
            I_L2IT = spike_L4E_at_t * w[0] + background_spikes  # Synaptic input current to L2IT
            V_L2IT, spike_L2IT = lif_update(V_L2IT, I_L2IT, dt, tau_m, V_reset, V_threshold)
            L2IT[:, t] = V_L2IT  # Membrane potential of L2IT
            for neuron_idx in spike_L2IT.nonzero(as_tuple=True)[0]:
                spikes_L2IT[neuron_idx].append(t * dt)  # Record the spike time

            # L3IT preparation (based on previous output)
            input_L3IT = prev_L5PT_spikes

            # Run the network for L3IT
            output_L3IT = snn_transition(input_L3IT.float() * w[1], W1_L3IT, W2_L3IT)
            spike_L3IT = output_L3IT.squeeze(0).squeeze(0).bool()  # Spike output of L3IT neurons
            L3IT[:, t] = spike_L3IT.float()
            for neuron_idx in spike_L3IT.nonzero(as_tuple=True)[0]:
                spikes_L3IT[neuron_idx].append(t * dt)

            # L3IT -> L5IT (modulated by L5PT output)
            background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[1]))
            I_L5IT = spike_L3IT * w[3] + background_spikes
            V_L5IT, spike_L5IT = lif_update(V_L5IT, I_L5IT, dt, tau_m, V_reset, V_threshold)
            L5IT[:, t] = V_L5IT  # Membrane potential of L5IT
            for neuron_idx in spike_L5IT.nonzero(as_tuple=True)[0]:
                spikes_L5IT[neuron_idx].append(t * dt)

            # L5IT -> L5PT (modulated by L2IT output)
            background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[2]))
            I_L5PT = spike_L5IT * w[4] * spike_L2IT + background_spikes - spike_L5PT_Inh_prev * w_Inh_to_L5PT
            V_L5PT, spike_L5PT = lif_update(V_L5PT, I_L5PT, dt, tau_m, V_reset, V_threshold)
            L5PT[:, t] = V_L5PT  # Membrane potential of L5PT
            for neuron_idx in spike_L5PT.nonzero(as_tuple=True)[0]:
                spikes_L5PT[neuron_idx].append(t * dt)

            # L5PT -> inhibitory neurons
            background_spikes_inh = torch.bernoulli(torch.full((num_neurons,), background_rate[4]))
            I_L5PT_Inh = spike_L5PT * w_L5PT_to_Inh + background_spikes_inh
            V_L5PT_Inh, spike_L5PT_Inh = lif_update(V_L5PT_Inh, I_L5PT_Inh, dt, tau_m, V_reset, V_threshold)
            L5PT_Inh[:, t] = V_L5PT_Inh  # Membrane potential of L5PT inhibitory neurons
            for neuron_idx in spike_L5PT_Inh.nonzero(as_tuple=True)[0]:
                spikes_L5PT_Inh[neuron_idx].append(t * dt)

            # Save inhibitory neuron spikes for the next time step
            spike_L5PT_Inh_prev = spike_L5PT_Inh

            # L5PT -> L6IT
            background_spikes = torch.bernoulli(torch.full((num_neurons,), background_rate[3]))
            I_L6IT = spike_L5PT * w[5] + background_spikes - spike_L6IT_Inh_prev * w_Inh_to_L6IT
            V_L6IT, spike_L6IT = lif_update(V_L6IT, I_L6IT, dt, tau_m, V_reset, V_threshold)
            L6IT[:, t] = V_L6IT  # Membrane potential of L6IT
            for neuron_idx in spike_L6IT.nonzero(as_tuple=True)[0]:
                spikes_L6IT[neuron_idx].append(t * dt)

            # Calculate average activity for L6IT
            mean_activity_L6IT = spike_L6IT.float().mean()

            # L6IT -> inhibitory neurons
            background_spikes_inh_L6IT = torch.bernoulli(torch.full((num_neurons,), background_rate[5]))
            I_L6IT_Inh = spike_L6IT * w_L6IT_to_Inh + background_spikes_inh_L6IT
            V_L6IT_Inh, spike_L6IT_Inh = lif_update(V_L6IT_Inh, I_L6IT_Inh, dt, tau_m, V_reset, V_threshold)
            L6IT_Inh[:, t] = V_L6IT_Inh  # Membrane potential of L6IT inhibitory neurons
            for neuron_idx in spike_L6IT_Inh.nonzero(as_tuple=True)[0]:
                spikes_L6IT_Inh[neuron_idx].append(t * dt)

            # Save L5PT output for the next time step
            prev_L5PT_spikes = spike_L5PT.unsqueeze(0).unsqueeze(0).float()
            # Save inhibitory neuron spikes for the next time step
            spike_L6IT_Inh_prev = spike_L6IT_Inh

        # Calculate average firing rates for each population
        firing_rate_L4E  = calculate_average_firing_rate_for_state(spikes_L4E, time_steps, dt)
        firing_rate_L2IT = calculate_average_firing_rate_for_state(spikes_L2IT, time_steps, dt)
        firing_rate_L3IT = calculate_average_firing_rate_for_state(spikes_L3IT, time_steps, dt)
        firing_rate_L5IT = calculate_average_firing_rate_for_state(spikes_L5IT, time_steps, dt)
        firing_rate_L5PT = calculate_average_firing_rate_for_state(spikes_L5PT, time_steps, dt)
        firing_rate_L6IT = calculate_average_firing_rate_for_state(spikes_L6IT, time_steps, dt)

        # Add results to the list
        firing_rate_L4E_results.append(firing_rate_L4E.numpy())
        firing_rate_L2IT_results.append(firing_rate_L2IT.numpy())
        firing_rate_L3IT_results.append(firing_rate_L3IT.numpy())
        firing_rate_L5IT_results.append(firing_rate_L5IT.numpy())
        firing_rate_L5PT_results.append(firing_rate_L5PT.numpy())
        firing_rate_L6IT_results.append(firing_rate_L6IT.numpy())

    # Score calculation section
    mse_list = []
    r2_list = []
    pearson_list = []

    # Repeat the simulation 10 times
    for iteration in range(10):
        # Generate matrix_6 and Ex_act based on previous logic
        matrix_6 = np.vstack([likelihood_results[iteration][:, 0], prediction_array_results[iteration][:, 0], likelihood_results[iteration][:, 0], predictiveprior_results[iteration][:, 0], post_results[iteration][:, 0], mapestimate_results[iteration][:, 0]]).T
        Ex_act = np.vstack([firing_rate_L4E_results[iteration], firing_rate_L3IT_results[iteration], firing_rate_L4E_results[iteration], firing_rate_L5IT_results[iteration], firing_rate_L5PT_results[iteration], firing_rate_L6IT_results[iteration]]).T

        # Standardize each column
        matrix_6 = (matrix_6 - np.mean(matrix_6, axis=0)) / np.std(matrix_6, axis=0)
        Ex_act = (Ex_act - np.mean(Ex_act, axis=0)) / np.std(Ex_act, axis=0)

        # Initialize matrices to store results
        mse_matrix = np.zeros((6, 6))
        r2_matrix = np.zeros((6, 6))
        pearson_matrix = np.zeros((6, 6))

        large_penalty_value = 1e6  # Penalty value for NaNs
        # Calculate metrics for each combination
        for i in range(6):
            for j in range(6):
                # Check for NaNs
                if np.any(np.isnan(matrix_6[:, i])) or np.any(np.isnan(Ex_act[:, j])):
                    # If NaNs are present, apply penalty
                    mse_matrix[i, j] = large_penalty_value
                    r2_matrix[i, j] = -large_penalty_value
                    pearson_matrix[i, j] = -large_penalty_value
                else:
                    # If no NaNs, calculate the metrics
                    mse_matrix[i, j] = mean_squared_error(matrix_6[:, i], Ex_act[:, j])
                    r2_matrix[i, j] = r2_score(matrix_6[:, i], Ex_act[:, j])
                    pearson_matrix[i, j], _ = pearsonr(matrix_6[:, i], Ex_act[:, j])

        # Add results for this iteration to the list
        mse_list.append(mse_matrix)
        r2_list.append(r2_matrix)
        pearson_list.append(pearson_matrix)

    # Calculate the average for each metric
    mse_avg = np.mean(mse_list, axis=0)
    r2_avg = np.mean(r2_list, axis=0)
    pearson_avg = np.mean(pearson_list, axis=0)

    # The score is the negative sum of the diagonal elements of the Pearson matrix
    score = -(np.trace(pearson_avg))

    return score

# Create and run the study
study = optuna.create_study()
study.optimize(objective, n_trials=100)

# Best parameters and score
study.best_params
study.best_value
study.best_trial


"""# Comparison between Potjans 2014 and Dynamic Bayesian Inference"""

# Import necessary libraries
import pandas as pd

# File path to the firing rates data
file_path = 'firing_rates_all.txt'

# Read the file into a DataFrame
df = pd.read_csv(file_path, delim_whitespace=True)

# Extract the even-numbered columns as a 1000x4 float matrix and remove the last unnecessary column
matrix_Potjans = df.iloc[:, 1::2].values
matrix_Potjans = matrix_Potjans[:, :-1]

# Reshape the matrix into 10x100x4
matrix_Potjans = matrix_Potjans.reshape(10, 100, 4)

# Lists to store the results for different metrics
mse_list = []
r2_list = []
pearson_list = []

# Run the simulation 10 times
for iteration in range(num_repeats):
    # Generate matrix_6 and Ex_act (based on the original logic)
    matrix_6 = np.vstack([likelihood_results[iteration][:, 0], 
                          prediction_array_results[iteration][:, 0], 
                          likelihood_results[iteration][:, 0], 
                          predictiveprior_results[iteration][:, 0], 
                          post_results[iteration][:, 0], 
                          mapestimate_results[iteration][:, 0]]).T
    Ex_act = matrix_Potjans[iteration]

    # Standardize each column
    matrix_6 = (matrix_6 - np.mean(matrix_6, axis=0)) / np.std(matrix_6, axis=0)
    Ex_act = (Ex_act - np.mean(Ex_act, axis=0)) / np.std(Ex_act, axis=0)

    # Initialize matrices to store the results for each iteration
    mse_matrix = np.zeros((6, 4))
    r2_matrix = np.zeros((6, 4))
    pearson_matrix = np.zeros((6, 4))

    # Calculate the metrics (MSE, R-Squared, Pearson Correlation) for each combination of the 6 (Bayesian variables) and 4 (Potjans layers)
    for i in range(6):
        for j in range(4):
            mse_matrix[i, j] = mean_squared_error(matrix_6[:, i], Ex_act[:, j])
            r2_matrix[i, j] = r2_score(matrix_6[:, i], Ex_act[:, j])
            pearson_matrix[i, j], _ = pearsonr(matrix_6[:, i], Ex_act[:, j])

    # Append results for each iteration to the list
    mse_list.append(mse_matrix)
    r2_list.append(r2_matrix)
    pearson_list.append(pearson_matrix)

# Calculate the average of each metric across all iterations
mse_avg = np.mean(mse_list, axis=0).T
r2_avg = np.mean(r2_list, axis=0).T
pearson_avg = np.mean(pearson_list, axis=0).T

# Custom labels for the axes
y_labels = ['L23E', 'L4E', 'L5E', 'L6E']
x_labels = ['Likelihood', 'Prediction', 'Likelihood', 'Predictive Prior', 'Posterior', 'MAP']

# Create the plots
fig, axes = plt.subplots(1, 3, figsize=(16, 6))

# MSE heatmap
sns.heatmap(mse_avg, ax=axes[0], cmap='coolwarm_r', annot=True, annot_kws={"size": 16}, vmin=0, vmax=1.6)
axes[0].set_xticklabels(x_labels, rotation=90, fontsize=20)
axes[0].set_yticklabels(y_labels, rotation=0, fontsize=20)
axes[0].set_title('Mean Squared Error (MSE)', fontsize=20)

# R-Squared heatmap
sns.heatmap(r2_avg, ax=axes[1], cmap='coolwarm', annot=True, annot_kws={"size": 16}, vmin=0, vmax=1)
axes[1].set_xticklabels(x_labels, rotation=90, fontsize=20)
axes[1].set_yticklabels(y_labels, rotation=0, fontsize=20)
axes[1].set_title('R-Squared (Explained Variance)', fontsize=20)

# Pearson Correlation heatmap
sns.heatmap(pearson_avg, ax=axes[2], cmap='coolwarm', annot=True, annot_kws={"size": 16}, vmin=0, vmax=1)
axes[2].set_xticklabels(x_labels, rotation=90, fontsize=20)
axes[2].set_yticklabels(y_labels, rotation=0, fontsize=20)
axes[2].set_title('Pearson Correlation', fontsize=20)

# Adjust layout and show the plots
plt.tight_layout()
plt.show()
